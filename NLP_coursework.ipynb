{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliyyah-u/NLP_Medical_NER/blob/main/NLP_coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IbzjCqsrupf0"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Aliyyah U.\"\n",
        "__version__ = \"IN3045 City St George's, University of London, Spring 2025\"\n",
        "#Ideally, we would reuse trusted libraries to perform NLP. Some of the commonly recommended ones are: , nltk, spacy, torchtext, huggingface: transformers, datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEAN"
      ],
      "metadata": {
        "id": "IzQOY-CJZXcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "from datasets import load_dataset\n",
        "!pip install -U huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "dataset = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "print(dataset)\n",
        "\n",
        "#see dataset samples\n",
        "print(dataset[\"train\"][0][\"tokens\"]) #can see that dataset is already tokenised\n",
        "print(dataset[\"train\"][0][\"ner_tags\"]) #can see that tags already in BIO format\n",
        "\n",
        "#split dataset for testing\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "print(dataset)\n",
        "\n",
        "#see contents of training dataset\n",
        "ner_feature = dataset[\"train\"].features\n",
        "print(ner_feature)\n",
        "\n",
        "#print sample of tokens with correct ner_tags\n",
        "words = dataset[\"train\"][0][\"tokens\"]\n",
        "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
        "\n",
        "def match_tokens_labels(tokens, labels):\n",
        "  line1 = \"\"\n",
        "  line2 = \"\"\n",
        "  for word, label in zip(words, labels):\n",
        "    max_length = max(len(word), len(label))\n",
        "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "    line2 += label + \" \" * (max_length - len(label) + 1)\n",
        "  print(line1)\n",
        "  print(line2)\n",
        "\n",
        "match_tokens_labels(words, labels)\n",
        "\n",
        "#convert dataset contents into lists for further processing\n",
        "df = dataset[\"train\"].to_pandas()\n",
        "train_tokens = df[\"tokens\"].tolist()\n",
        "train_tags = df[\"ner_tags\"].tolist()\n",
        "\n",
        "print(\"\\nSample of training tokens:\")\n",
        "print(train_tokens[:2])\n",
        "print(\"\\nSample of training tags:\")\n",
        "print(train_tags[:2])\n",
        "\n",
        "# See all unique tag names\n",
        "train_unique_tags = set(tag for sublist in train_tags for tag in sublist)\n",
        "print(\"\\nAll unique NER tags in train dataset:\")\n",
        "print(sorted(train_unique_tags))"
      ],
      "metadata": {
        "id": "6CIiSsAFZ8vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import random\n",
        "\n",
        "# Load a real BPE tokenizer (GPT-2 uses BPE)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "\n",
        "# Build the list of unique NER tags from the dataset\n",
        "ner_tags = list(set(tag for example in dataset[\"train\"][\"ner_tags\"] for tag in example))\n",
        "ner_tags.sort()  # Optional: sort for consistency\n",
        "print(f\"NER Tags: {ner_tags}\")\n",
        "\n",
        "# Function to tokenize using internal BPE merges (now using tokenizer methods)\n",
        "def tokenize(text):\n",
        "    # Tokenize the text with GPT-2 tokenizer\n",
        "    bpe_tokens = tokenizer.tokenize(text)\n",
        "    return bpe_tokens\n",
        "\n",
        "# Assign random NER tags to BPE tokens\n",
        "def generate_random_bpe_ner(tokens):\n",
        "    text = \" \".join(tokens)\n",
        "    bpe_tokens = tokenize(text)\n",
        "    random_tags = [random.choice(ner_tags) for _ in bpe_tokens]\n",
        "    return bpe_tokens, random_tags\n",
        "\n",
        "# Pretty-print function\n",
        "def match_tokens_labels(tokens, labels):\n",
        "    line1 = \"\"\n",
        "    line2 = \"\"\n",
        "    for word, label in zip(tokens, labels):\n",
        "        max_length = max(len(word), len(label))\n",
        "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
        "    print(line1)\n",
        "    print(line2)\n",
        "\n",
        "# Example usage\n",
        "mytokens = dataset[\"train\"][0][\"tokens\"]\n",
        "bpe_tokens, bpe_labels = generate_random_bpe_ner(mytokens)\n",
        "match_tokens_labels(bpe_tokens, bpe_labels)"
      ],
      "metadata": {
        "id": "sNQiPt4erY9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "!pip install -U huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "\n",
        "dataset = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "print(dataset[\"train\"][0][\"tokens\"])\n",
        "print(dataset[\"train\"][0][\"ner_tags\"])\n",
        "\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "print(dataset)\n",
        "\n",
        "ner_feature = dataset[\"train\"].features\n",
        "print(ner_feature)\n",
        "\n",
        "def match_tokens_labels(tokens, labels):\n",
        "  line1 = \"\"\n",
        "  line2 = \"\"\n",
        "  for word, label in zip(words, labels):\n",
        "    max_length = max(len(word), len(label))\n",
        "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "    line2 += label + \" \" * (max_length - len(label) + 1)\n",
        "  print(line1)\n",
        "  print(line2)\n",
        "\n",
        "words = dataset[\"train\"][0][\"tokens\"]\n",
        "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
        "match_tokens_labels(words, labels)\n",
        "\n",
        "# fyi - View all tags\n",
        "train_unique_tags = set(tag for sublist in train_tags for tag in sublist)\n",
        "print(\"\\nAll unique training NER tags:\")\n",
        "print(sorted(train_unique_tags))\n",
        "\n",
        "import random\n",
        "\n",
        "# Function to match tokens with random NER labels\n",
        "def match_tokens_labels(tokens, train_tags):\n",
        "    # Create the list of unique tags\n",
        "    all_unique_tags = sorted(set(tag for sublist in train_tags for tag in sublist))\n",
        "\n",
        "    # Use random.choices() to assign random tags (with replacement) to each token\n",
        "    random_tags = random.choices(all_unique_tags, k=len(tokens))  # Allow repeated sampling\n",
        "\n",
        "    # Prepare strings for printing\n",
        "    line1 = \"\"\n",
        "    line2 = \"\"\n",
        "\n",
        "    for word, label in zip(tokens, random_tags):\n",
        "        max_length = max(len(word), len(label))\n",
        "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
        "\n",
        "    # Print the tokens and their randomly assigned labels\n",
        "    print(line1)\n",
        "    print(line2)\n",
        "\n",
        "# Example usage (make sure you pass tokens and the train_tags from your dataset)\n",
        "mytokens = dataset[\"train\"][0][\"tokens\"]\n",
        "train_tags = dataset[\"train\"][\"ner_tags\"]\n",
        "\n",
        "match_tokens_labels(mytokens, train_tags)"
      ],
      "metadata": {
        "id": "RGgUOLWCxDE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MESSY\n"
      ],
      "metadata": {
        "id": "1iT9A7OMZakm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load dataset\n"
      ],
      "metadata": {
        "id": "xSfjsptclwBq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uqXCa343FdI"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade huggingface_hub\n",
        "\n",
        "# Login to access dataset\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_parquet(\"hf://datasets/parsa-mhmdi/Medical_NER/data/train-00000-of-00001.parquet\")\n",
        "\n",
        "train_indices, test_indices = train_test_split(df.index, test_size=0.2, random_state=42)\n",
        "\n",
        "train_data = df.loc[train_indices]\n",
        "test_data = df.loc[test_indices]\n",
        "\n",
        "# Preview samples from train and test, check df is correctly loaded\n",
        "print(\"Train sample:\")\n",
        "print(train_data.head())\n",
        "print(\"\\nTest sample:\")\n",
        "print(test_data.head())\n",
        "\n",
        "# Check data types of 'tokens' and 'ner_tags' columns\n",
        "print(\"\\nData type of train_data.tokens:\")\n",
        "print(type(train_data['tokens']))\n",
        "print(\"\\nData type of train_data.ner_tags:\")\n",
        "print(type(train_data['ner_tags']))\n",
        "\n",
        "# Convert to list\n",
        "train_tokens = train_data.tokens.tolist()\n",
        "train_tags = train_data.ner_tags.tolist()\n",
        "print(\"\\nSample of training tokens:\")\n",
        "print(train_tokens[:5])\n",
        "print(\"\\nSample of training tags:\")\n",
        "print(train_tags[:5])\n",
        "\n",
        "# fyi - View all tags\n",
        "train_unique_tags = set(tag for sublist in train_tags for tag in sublist)\n",
        "print(\"\\nAll unique training NER tags:\")\n",
        "print(sorted(train_unique_tags))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part of Speech Tagging"
      ],
      "metadata": {
        "id": "LajU_VEEFqVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(df[tokens])\n",
        "\n",
        "for token in doc:\n",
        "  print(token, \"|\", token.pos_)"
      ],
      "metadata": {
        "id": "36YqpzN3DVh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lab"
      ],
      "metadata": {
        "id": "w3S2bRw37NeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple tokeniser"
      ],
      "metadata": {
        "id": "c4-7fLNo_1kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_parquet(\"hf://datasets/parsa-mhmdi/Medical_NER/data/train-00000-of-00001.parquet\")\n",
        "print('\\n',df.info())\n",
        "\n",
        "#split for test dataset\n",
        "train_indices, test_indices = train_test_split(df.index, test_size=0.2, random_state=42)\n",
        "train_data = df.loc[train_indices]\n",
        "test_data = df.loc[test_indices]\n",
        "\n",
        "print(\"\\nTrain token sample:\")\n",
        "print(train_data.tokens[:5]) #dataset is already tokenised on whitespace/capital letters, & normalised by lowercasing"
      ],
      "metadata": {
        "id": "Db56cl65EzXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "meHazuqZAPVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_stemmer(token):\n",
        "    # we will only work over words > 2 characters.\n",
        "    if len(token) <= 2:\n",
        "        return token\n",
        "\n",
        "    suffixes = ['al', 'ance', 'ence', 'able', 'ible', 'ment', 'ant', 'ent', 'ism',\n",
        "                'ate', 'iti', 'ous', 'ive', 'ize']\n",
        "\n",
        "    for suffix in suffixes:\n",
        "        if token.endswith(suffix):\n",
        "            token = token[:-len(suffix)]\n",
        "            break # TODO: why are we breaking here? (there are better ways of doing this)\n",
        "\n",
        "    return token\n",
        "\n",
        "# test this function for \"capital\"\n",
        "print(simple_stemmer(train_data.tokens))"
      ],
      "metadata": {
        "id": "8FN3ei6hATrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTest sample:\")\n",
        "print(test_data.head())\n",
        "\n",
        "# Check data types of 'tokens' and 'ner_tags' columns\n",
        "print(\"\\nData type of train_data.tokens:\")\n",
        "print(type(train_data['tokens']))\n",
        "print(\"\\nData type of train_data.ner_tags:\")\n",
        "print(type(train_data['ner_tags']))\n",
        "\n",
        "# Convert to list\n",
        "train_tokens = train_data.tokens.tolist()\n",
        "train_tags = train_data.ner_tags.tolist()\n",
        "print(\"\\nSample of training tokens:\")\n",
        "print(train_tokens[:20])\n",
        "print(\"\\nSample of training tags:\")\n",
        "print(train_tags[:5])\n",
        "\n",
        "# fyi - View all tags\n",
        "train_unique_tags = set(tag for sublist in train_tags for tag in sublist)\n",
        "print(\"\\nAll unique training NER tags:\")\n",
        "print(sorted(train_unique_tags))"
      ],
      "metadata": {
        "id": "4tJbaHQk_Hyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "todo\n",
        "\n",
        "lexical stage\n",
        "Morphological analysis procedures\n",
        "include suf fix normalization, stemming and root\n",
        "word lookup and analysis of internal p,mc~itation.\n",
        "\n",
        "text-processing steps such as\n",
        " POS tagging, morphological analysis, and gazetteer\n",
        " matching."
      ],
      "metadata": {
        "id": "6bp8YzdRKMiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Byte pair encoding (BPE)"
      ],
      "metadata": {
        "id": "phVw6_PEEqIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import collections\n",
        "\n",
        "def compute_symbol_pairs_frequency(vocab):\n",
        "    #Compute the frequency of adjacent symbol pairs in the vocabulary.\n",
        "\n",
        "    symbol_pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            symbol_pairs[symbols[i], symbols[i+1]] += freq\n",
        "    return symbol_pairs\n",
        "\n",
        "def merge_vocabulary(symbol_pair, vocab_in):\n",
        "    #Merge the given symbol pair in the vocabulary.\n",
        "\n",
        "    vocab_out = {}\n",
        "    bigram = re.escape(' '.join(symbol_pair))\n",
        "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in vocab_in:\n",
        "        word_out = pattern.sub(''.join(symbol_pair), word)\n",
        "        vocab_out[word_out] = vocab_in[word]\n",
        "    return vocab_out\n",
        "\n",
        "def extract_symbol_pairs(word):\n",
        "    #Return a set of symbol pairs in a word.\n",
        "\n",
        "    symbol_pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        symbol_pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return symbol_pairs\n",
        "\n",
        "train_data = {'n a t u r a l </w>': 5, 'n a t u r e </w>': 2, 'l a n g u a g e </w>': 6, 'l a n g u a g e s </w>': 3}\n",
        "\n",
        "bpe_codes = {}\n",
        "bpe_codes_reverse = {}\n",
        "\n",
        "num_merges = 10\n",
        "\n",
        "for i in range(num_merges):\n",
        "    pairs = compute_symbol_pairs_frequency(train_data)\n",
        "    best_pair = max(pairs, key=pairs.get)\n",
        "    train_data = merge_vocabulary(best_pair, train_data)\n",
        "\n",
        "    bpe_codes[best_pair] = i\n",
        "    bpe_codes_reverse[best_pair[0] + best_pair[1]] = best_pair\n",
        "\n",
        "    print(\"new merge: {}\".format(best_pair))\n",
        "    print(\"train data: {}\".format(train_data))"
      ],
      "metadata": {
        "id": "X9XMPmf8EqmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tutorial"
      ],
      "metadata": {
        "id": "iV8U867kJoJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "140No_0ZJmlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp('Dr. Strange loves pav bhaji of mumbai as it costs only £2 per plate')\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "id": "UrJMGNhtLRUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0]"
      ],
      "metadata": {
        "id": "Bym_JN-BLyYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "span = doc[1:5]\n",
        "print(span)"
      ],
      "metadata": {
        "id": "DuxtIwT0MHO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token0 = doc[0]\n",
        "token0"
      ],
      "metadata": {
        "id": "ZWZVHZ7CMOHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(token0)"
      ],
      "metadata": {
        "id": "HARk62LeMr-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir(token0)"
      ],
      "metadata": {
        "id": "dNhjIzw6MfCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token0.like_num"
      ],
      "metadata": {
        "id": "dNBNoUdoMzUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token0.is_alpha"
      ],
      "metadata": {
        "id": "o8QCabnlM-gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## huggingface tutorial"
      ],
      "metadata": {
        "id": "XXIk17gpNyn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import pipeline\n",
        "\n",
        "!pip install datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"nyu-mll/glue\", \"mrpc\", split=\"train\")\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def encode(examples):\n",
        "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "dataset = dataset.map(encode, batched=True)\n",
        "dataset[0]\n",
        "\n",
        "dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n",
        "\n",
        "import torch\n",
        "\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "qEStm8BjN2yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP pipeline with huggingface dataset tutorial"
      ],
      "metadata": {
        "id": "UhAmLkseSFAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load dataset"
      ],
      "metadata": {
        "id": "KLWaWe_BlWaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "\n",
        "ds_keys = dataset.keys()\n",
        "print(ds_keys)\n",
        "\n",
        "ds_size = dataset['train'].dataset_size\n",
        "print(ds_size)\n",
        "\n",
        "ds_desc = dataset['train'].description\n",
        "print(ds_desc)\n",
        "\n",
        "ds_features = dataset['train'].features\n",
        "print(ds_features)\n",
        "\n",
        "ds_sample = dataset['train'][0:1]\n",
        "print(ds_sample)\n",
        "\n",
        "dataset['train'].train_test_split(test_size=0.1)"
      ],
      "metadata": {
        "id": "yLq1n1vbSLQP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w3S2bRw37NeL"
      ],
      "authorship_tag": "ABX9TyMO70IwcUigPQrQniFmn6mm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}