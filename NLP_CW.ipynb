{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliyyah-u/NLP_Medical_NER/blob/main/NLP_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load & Preprocess Dataset, Tokenisation, Baseline Model"
      ],
      "metadata": {
        "id": "rlfe_ut6AfJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets spacy evaluate\n",
        "!pip install -q transformers torch\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "from datasets import load_dataset\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import AutoTokenizer, DataCollatorForTokenClassification\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "ds = load_dataset(\"rjac/biobert-ner-diseases-dataset\")\n",
        "print(\"\\nInitial dataset:\",ds)\n",
        "\n",
        "# extract label names from tag values\n",
        "label_list = ds[\"train\"].features[\"tags\"].feature.names\n",
        "\n",
        "# NER tag distribution in training set\n",
        "all_labels = [tag for seq in ds[\"train\"][\"tags\"] for tag in seq]\n",
        "label_counts = Counter(all_labels)\n",
        "print(\"\\nNER tag distribution (train):\")\n",
        "for i, count in label_counts.items():\n",
        "    print(label_list[i]+ \":\" + str(count))\n",
        "\n",
        "# preprocessing (POS tagging)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"lemmatizer\"])\n",
        "\n",
        "def add_pos_tags(batch):\n",
        "    texts = [\" \".join(tokens) for tokens in batch[\"tokens\"]]\n",
        "    batch[\"pos_tags\"] = []\n",
        "    for doc in nlp.pipe(texts):\n",
        "        batch[\"pos_tags\"].append([token.pos_ for token in doc])\n",
        "    return batch\n",
        "\n",
        "ds = ds.map(add_pos_tags, batched=True)\n",
        "\n",
        "all_pos_tags = [tag for sublist in ds[\"train\"][\"pos_tags\"] for tag in sublist]\n",
        "pos_counts = Counter(all_pos_tags)\n",
        "print(\"\\nPOS Tag Distribution in training dataset:\")\n",
        "print(pos_counts.most_common())\n",
        "\n",
        "for tag in set(all_pos_tags):\n",
        "    print(tag + \": \" + str(spacy.explain(tag)))\n",
        "\n",
        "print(\"Dataset sample:\")\n",
        "print({\n",
        "    \"tokens\": ds[\"train\"][0][\"tokens\"][:5],\n",
        "    \"tags\": ds[\"train\"][0][\"tags\"][:5],\n",
        "    \"pos_tags\": ds[\"train\"][0][\"pos_tags\"][:5]\n",
        "})\n",
        "\n",
        "# Tokenisation\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, padding=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "\n",
        "    for i, mylabel in enumerate(examples[\"tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(mylabel[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# baseline model\n",
        "ds[\"test\"] = ds[\"test\"].add_column(\"baseline_pred\", [[0]*len(tokens) for tokens in ds[\"test\"][\"tokens\"]])\n",
        "\n",
        "print(\"\\nBaseline sample predictions:\")\n",
        "sample_df = pd.DataFrame({\n",
        "    \"Token\": ds[\"test\"][\"tokens\"][0],\n",
        "    \"POS tag\": ds[\"test\"][\"pos_tags\"][0],\n",
        "    \"True\": [label_list[t] for t in ds[\"test\"][\"tags\"][0]],\n",
        "    \"Pred\": [label_list[p] for p in ds[\"test\"][\"baseline_pred\"][0]]\n",
        "}).head(20)\n",
        "print(sample_df)\n",
        "\n",
        "print(\"\\nBaseline Classification Report:\")\n",
        "true_tags = [t for tags in ds[\"test\"][\"tags\"] for t in tags]\n",
        "pred_tags = [p for pred in ds[\"test\"][\"baseline_pred\"] for p in pred]\n",
        "print(classification_report(true_tags, pred_tags, target_names=label_list, zero_division=0, digits=4))"
      ],
      "metadata": {
        "id": "-YgOjfDIA3WR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple CRF Model"
      ],
      "metadata": {
        "id": "sbR0WbASD-V2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sklearn-crfsuite\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import CRF\n",
        "from sklearn_crfsuite import metrics\n",
        "\n",
        "# My feature dictionary for all tokens\n",
        "def extract_token_features(token, pos):\n",
        "    return {\n",
        "        \"word.lower()\": token.lower(),\n",
        "        \"word.isupper()\": token.isupper(),\n",
        "        \"word.istitle()\": token.istitle(),\n",
        "        \"word.isdigit()\": token.isdigit(),\n",
        "        \"postag\": pos,\n",
        "    }\n",
        "\n",
        "def sent_to_features(tokens, pos_tags):\n",
        "    return [extract_token_features(tok, pos) for tok, pos in zip(tokens, pos_tags)]\n",
        "\n",
        "# method to extract label names from tag values\n",
        "def sent_to_labels(tag_ids):\n",
        "    return [label_list[tag] for tag in tag_ids]\n",
        "\n",
        "# prepare data\n",
        "X_train = [sent_to_features(tokens, pos_tags) for tokens, pos_tags in zip(ds[\"train\"][\"tokens\"], ds[\"train\"][\"pos_tags\"])]\n",
        "y_train = [sent_to_labels(tags) for tags in ds[\"train\"][\"tags\"]] # convert id2str\n",
        "\n",
        "X_test = [sent_to_features(tokens, pos_tags) for tokens, pos_tags in zip(ds[\"test\"][\"tokens\"], ds[\"test\"][\"pos_tags\"])]\n",
        "y_test = [sent_to_labels(tags) for tags in ds[\"test\"][\"tags\"]]\n",
        "\n",
        "# CRF model\n",
        "crf = CRF()\n",
        "crf.fit(X_train, y_train)\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "print(\"\\nCRF Classification Report:\")\n",
        "print(metrics.flat_classification_report(y_test, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "AM-V_paKHEfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBERT Model"
      ],
      "metadata": {
        "id": "HCPY6D38QMQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForTokenClassification, DistilBertTokenizerFast, Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# use GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=len(label_list)).to(device)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "train_dataset = tokenized_ds[\"train\"]\n",
        "eval_dataset = tokenized_ds[\"test\"]\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_output\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator = data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(\"Eval Results:\", results)\n",
        "\n",
        "predictions, label_ids, _ = trainer.predict(eval_dataset)\n",
        "pred_labels = np.argmax(predictions, axis=2)\n",
        "\n",
        "true_labels = [[label_list[l] for l in label_ids[i]] for i in range(len(label_ids))]\n",
        "pred_labels = [[label_list[l] for l in pred_labels[i]] for i in range(len(pred_labels))]\n",
        "\n",
        "# flat_true = [item for sublist in true_labels for item in sublist]\n",
        "# flat_pred = [item for sublist in pred_labels for item in sublist]\n",
        "# print(classification_report(flat_true, flat_pred, digits=4))\n",
        "\n",
        "flat_true = []\n",
        "flat_pred = []\n",
        "\n",
        "for i in range(len(label_ids)):\n",
        "    for j in range(len(label_ids[i])):\n",
        "        if label_ids[i][j] != -100:\n",
        "            flat_true.append(label_list[label_ids[i][j]])\n",
        "            flat_pred.append(pred_labels[i][j])\n",
        "\n",
        "print(classification_report(flat_true, flat_pred, digits=4))\n",
        "\n",
        "# save model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path = \"/content/drive/My Drive/Colab Notebooks/CW_MedNER_UG/distilbert_ner_model\"\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(\"Model saved to:\" + str(save_path))\n",
        "\n",
        "# uncomment code below to load model:\n",
        "# from transformers import DistilBertForTokenClassification, DistilBertTokenizerFast\n",
        "# model = DistilBertForTokenClassification.from_pretrained(save_path)\n",
        "# tokenizer = DistilBertTokenizerFast.from_pretrained(save_path)"
      ],
      "metadata": {
        "id": "bzF8qmpMS7wD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdN3hitKx2maddw6PPK3Ro",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}