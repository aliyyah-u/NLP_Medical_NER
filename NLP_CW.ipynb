{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliyyah-u/NLP_Medical_NER/blob/main/NLP_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER PIPELINE"
      ],
      "metadata": {
        "id": "-yV5o1IwciAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOAD DATASET & TOKENISATION"
      ],
      "metadata": {
        "id": "0f0ZHvcSstrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate seqeval\n",
        "\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "import evaluate\n",
        "\n",
        "ds = load_dataset(\"rjac/biobert-ner-diseases-dataset\")\n",
        "train_ds = ds[\"train\"]\n",
        "test_ds = ds[\"test\"]\n",
        "\n",
        "print(ds)\n",
        "print(train_ds[0])\n",
        "print(ds[\"train\"].features[\"tags\"].feature.names)\n",
        "\n",
        "label_list = ds[\"train\"].features[f\"tags\"].feature.names\n",
        "print(label_list)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "example = ds[\"train\"][0]\n",
        "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "print(tokens)\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "# Evaluation setup\n",
        "seqeval = evaluate.load(\"seqeval\")\n",
        "\n",
        "labels = [label_list[i] for i in example[f\"tags\"]]\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "# Label mappings\n",
        "id2label = {\n",
        "    0: \"O\",\n",
        "    1: \"B-Disease\",\n",
        "    2: \"I-Disease\",\n",
        "    }\n",
        "\n",
        "label2id = {\n",
        "    \"O\": 0,\n",
        "    \"B-Disease\": 1,\n",
        "    \"I-Disease\": 2,\n",
        "}"
      ],
      "metadata": {
        "id": "Gc0WLUnPcoP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREPROCESS DATASET (POS TAGGING)"
      ],
      "metadata": {
        "id": "-2kPT0ApZAmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def add_pos_tags_to_dataset(examples):\n",
        "    # Join tokens for SpaCy input\n",
        "    texts = [\" \".join(tokens) for tokens in examples[\"tokens\"]]\n",
        "    docs = list(nlp.pipe(texts))\n",
        "    pos_tags = [[token.pos_ for token in doc] for doc in docs]\n",
        "    examples[\"pos_tags\"] = pos_tags\n",
        "    return examples\n",
        "\n",
        "train_ds = train_ds.map(add_pos_tags_to_dataset, batched=True)\n",
        "print(train_ds[0])"
      ],
      "metadata": {
        "id": "FzjpgAS1ZSRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BASELINES"
      ],
      "metadata": {
        "id": "ZGBNo08Ev2bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CRF\n",
        "\n",
        "import nltk\n",
        "import sklearn_crfsuite\n",
        "import eli5\n",
        "\n",
        "train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
        "test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
        "train_sents[0]\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "    features = {\n",
        "        'bias': 1.0,\n",
        "        'word.lower()': word.lower(),\n",
        "        'word[-3:]': word[-3:],\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "        'postag': postag,\n",
        "        'postag[:2]': postag[:2],\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            '-1:word.lower()': word1.lower(),\n",
        "            '-1:word.istitle()': word1.istitle(),\n",
        "            '-1:word.isupper()': word1.isupper(),\n",
        "            '-1:postag': postag1,\n",
        "            '-1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            '+1:word.lower()': word1.lower(),\n",
        "            '+1:word.istitle()': word1.istitle(),\n",
        "            '+1:word.isupper()': word1.isupper(),\n",
        "            '+1:postag': postag1,\n",
        "            '+1:postag[:2]': postag1[:2],\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]\n",
        "\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_test = [sent2features(s) for s in test_sents]\n",
        "y_test = [sent2labels(s) for s in test_sents]\n",
        "\n",
        "X_train[0][1]\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=20,\n",
        "    all_possible_transitions=False,\n",
        ")\n",
        "crf.fit(X_train, y_train);\n",
        "\n",
        "eli5.show_weights(crf, top=30)\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=200,\n",
        "    c2=0.1,\n",
        "    max_iterations=20,\n",
        "    all_possible_transitions=False,\n",
        ")\n",
        "crf.fit(X_train, y_train)\n",
        "eli5.show_weights(crf, top=30)\n",
        "\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=20,\n",
        "    all_possible_transitions=True,\n",
        ")\n",
        "crf.fit(X_train, y_train);\n",
        "\n",
        "eli5.show_weights(crf, top=5, show=['transition_features'])\n",
        "\n",
        "eli5.show_weights(crf, top=10, targets=['O', 'B-ORG', 'I-ORG'])\n",
        "\n",
        "eli5.show_weights(crf, top=10, feature_re='^word\\.is',\n",
        "                  horizontal_layout=False, show=['targets'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "On5EY8pRv5iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BiLSTM-CRF\n",
        "\n"
      ],
      "metadata": {
        "id": "km2lOJK0wdq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert/distilbert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "faGOEysgm5J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing & baseline (all predicted tags are labelled as 'Other')"
      ],
      "metadata": {
        "id": "ritsx2wVwME3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install -U datasets huggingface_hub\n",
        "\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "login()\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "print('\\nDATASET FEATURES:\\n', dataset)\n",
        "\n",
        "# Show a dataset sample\n",
        "print('\\nA DATASET SAMPLE:')\n",
        "print(dataset[\"train\"][0][\"tokens\"])    # Text is already tokenised\n",
        "print(dataset[\"train\"][0][\"ner_tags\"])  # NER tags (already in BIO format)\n",
        "\n",
        "# Split dataset for testing\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "print('\\nTHE SPLIT DATASET FEATURES:\\n', dataset)\n",
        "\n",
        "# Check column types\n",
        "ner_feature = dataset[\"train\"].features\n",
        "print('\\nDATA TYPES:\\n', ner_feature)\n",
        "\n",
        "# Convert dataset contents into lists for processing\n",
        "train_tokens = dataset[\"train\"][\"tokens\"]\n",
        "train_tags = dataset[\"train\"][\"ner_tags\"]\n",
        "\n",
        "# View samples\n",
        "print(\"\\nSAMPLE TRAINING TOKENS:\")\n",
        "print(train_tokens[0])\n",
        "print(\"\\nSAMPLE TRAINING TAGS:\")\n",
        "print(train_tags[0])\n",
        "\n",
        "# See all unique tag values\n",
        "train_unique_tags = set(tag for sublist in train_tags for tag in sublist)\n",
        "print(\"\\nALL UNIQUE NER TAGS IN TRAINING SET:\")\n",
        "print(sorted(train_unique_tags))\n",
        "print(f\"\\nNumber of unique NER tags in the training set: {len(train_unique_tags)}\")\n",
        "\n",
        "# Function to generate baseline predicted tags\n",
        "def add_predicted_tags(tokens, tags):\n",
        "    return [['Other'] * len(token_list) for token_list in tags]\n",
        "\n",
        "# Remove 'pred_ner_tags' column if already exists\n",
        "if 'pred_ner_tags' in dataset[\"train\"].column_names:\n",
        "    dataset[\"train\"] = dataset[\"train\"].remove_columns(\"pred_ner_tags\")\n",
        "\n",
        "# Generate and add predicted NER tags\n",
        "predicted_train_tags = add_predicted_tags(train_tokens, train_tags)\n",
        "dataset[\"train\"] = dataset[\"train\"].add_column(\"pred_ner_tags\", predicted_train_tags)\n",
        "\n",
        "# Dataframe for organised display\n",
        "df = dataset[\"train\"].to_pandas()\n",
        "\n",
        "# Baseline model (all predicted tags as 'Other')\n",
        "def match_tokens_labels(tokens, true_tags, pred_tags):\n",
        "    df_display = pd.DataFrame({\n",
        "        \"Token\": tokens,\n",
        "        \"True Tag\": true_tags,\n",
        "        \"Pred Tag\": pred_tags\n",
        "    })\n",
        "    print(\"\\nSAMPLE OF TOKENS WITH TRUE AND PREDICTED NER TAGS\\n\")\n",
        "    print(df_display.head(20))\n",
        "\n",
        "# Show first training example\n",
        "match_tokens_labels(df[\"tokens\"][0], df[\"ner_tags\"][0], df[\"pred_ner_tags\"][0])\n",
        "\n",
        "# Flatten true and predicted tags\n",
        "true_tags = [tag for sublist in df[\"ner_tags\"] for tag in sublist]\n",
        "pred_tags = [tag for sublist in df[\"pred_ner_tags\"] for tag in sublist]\n",
        "\n",
        "print(classification_report(true_tags, pred_tags))\n",
        "print(f1_score(true_tags, pred_tags, average='macro'))"
      ],
      "metadata": {
        "id": "L5B4ogGCuk3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DistilBERT (Model 2)"
      ],
      "metadata": {
        "id": "OKQsD4Dg04u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U accelerate\n",
        "!pip install -U datasets\n",
        "!pip install -U huggingface_hub"
      ],
      "metadata": {
        "id": "m-EmnN9YLGbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "sC8N2Q8sLtMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "import numpy as np\n",
        "\n",
        "login()\n",
        "\n",
        "data = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "data"
      ],
      "metadata": {
        "id": "o8kcpvFCUWpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'].features"
      ],
      "metadata": {
        "id": "DzCMkESpVUdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][0]['ner_tags']"
      ],
      "metadata": {
        "id": "exMlqe4hRFah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract all unique NER tag strings from the training set\n",
        "all_tag_strings = [tag for example in data['train']['ner_tags'] for tag in example]\n",
        "unique_tags = sorted(set(all_tag_strings))\n",
        "\n",
        "# Step 2: Create mappings\n",
        "label_to_id = {label: idx for idx, label in enumerate(unique_tags)}\n",
        "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
        "\n",
        "print(\"Label to ID mapping:\", label_to_id)"
      ],
      "metadata": {
        "id": "n3rr_n3FRjRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Convert one sample's ner_tags to integer IDs\n",
        "string_tags = data['train'][0]['ner_tags']\n",
        "tag_ids = [label_to_id[tag] for tag in string_tags]\n",
        "\n",
        "print(tag_ids)"
      ],
      "metadata": {
        "id": "7NL13gZ2R3Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all unique NER tag strings\n",
        "all_tags = [tag for example in data['train']['ner_tags'] for tag in example]\n",
        "unique_tags = sorted(set(all_tags))\n",
        "\n",
        "# Create mapping from label to ID\n",
        "label_to_id = {label: idx for idx, label in enumerate(unique_tags)}\n",
        "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
        "\n",
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id is None:\n",
        "            new_labels.append(-100)\n",
        "        elif word_id != current_word:\n",
        "            current_word = word_id\n",
        "            label = labels[word_id]\n",
        "            label_id = label_to_id[label]\n",
        "            new_labels.append(label_id)\n",
        "        else:\n",
        "            label = labels[word_id]\n",
        "            label_id = label_to_id[label]\n",
        "            # Optionally adjust label for I-type\n",
        "            if \"B-\" in label:\n",
        "                label_id = label_to_id[label.replace(\"B-\", \"I-\")]\n",
        "            new_labels.append(label_id)\n",
        "    return new_labels"
      ],
      "metadata": {
        "id": "P2CX4CEqSN0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(data['train'][:])[['tokens', 'ner_tags']].iloc[0]"
      ],
      "metadata": {
        "id": "cHerwnJ8VmdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model building"
      ],
      "metadata": {
        "id": "uBckpSVPYkvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean"
      ],
      "metadata": {
        "id": "XDErvRyXVQje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate datasets huggingface_hub\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "login()\n",
        "\n",
        "data = load_dataset(\"parsa-mhmdi/Medical_NER\")"
      ],
      "metadata": {
        "id": "LnQDydVhLeZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)\n",
        "print(data['train'].features)\n",
        "print(data['train'][0]['tokens'])\n",
        "print(data['train'][0]['ner_tags'])  # These are strings (e.g. \"B-CHEMICAL\")\n",
        "\n",
        "# Step 1: Extract unique string labels and build mappings\n",
        "all_tag_strings = [tag for example in data['train']['ner_tags'] for tag in example]\n",
        "unique_tags = sorted(set(all_tag_strings))\n",
        "\n",
        "label_to_id = {label: idx for idx, label in enumerate(unique_tags)}\n",
        "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
        "\n",
        "print(\"Label to ID mapping:\", label_to_id)\n",
        "\n",
        "# Step 2: Define label alignment function\n",
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id is None:\n",
        "            new_labels.append(-100)\n",
        "        elif word_id != current_word:\n",
        "            current_word = word_id\n",
        "            label = labels[word_id]\n",
        "            label_id = label_to_id[label]\n",
        "            new_labels.append(label_id)\n",
        "        else:\n",
        "            label = labels[word_id]\n",
        "            label_id = label_to_id[label]\n",
        "            # Optional: if the word is split, convert B- to I-\n",
        "            if label.startswith(\"B-\"):\n",
        "                i_label = label.replace(\"B-\", \"I-\")\n",
        "                label_id = label_to_id.get(i_label, label_id)\n",
        "            new_labels.append(label_id)\n",
        "    return new_labels\n",
        "\n",
        "# Step 3: Load tokenizer\n",
        "model_checkpoint = \"distilbert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "assert tokenizer.is_fast  # must be fast tokenizer\n",
        "\n",
        "# Step 4: Tokenize and align labels\n",
        "def tokenize_and_align(example):\n",
        "    tokenized = tokenizer(example['tokens'], is_split_into_words=True, truncation=True)\n",
        "    word_ids = tokenized.word_ids()\n",
        "    tokenized['labels'] = align_labels_with_tokens(example['ner_tags'], word_ids)\n",
        "    return tokenized\n",
        "\n",
        "# Apply to dataset\n",
        "data = data.map(tokenize_and_align)\n",
        "\n",
        "# Preview sample tokenized entry\n",
        "print(data['train'][0])"
      ],
      "metadata": {
        "id": "05heAc-yWtUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(data['train'][:])[['tokens','ner_tags']].iloc[0]"
      ],
      "metadata": {
        "id": "mxHsu2v-YLmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_numeric_tags(example):\n",
        "    example['ner_tags_num'] = [label_to_id[tag] for tag in example['ner_tags']]\n",
        "    return example\n",
        "\n",
        "data = data.map(add_numeric_tags)\n",
        "\n",
        "print(data['train'][0]['ner_tags'])      # Original string labels\n",
        "print(data['train'][0]['ner_tags_num'])  # Integer-mapped labels\n",
        "\n",
        "pd.DataFrame(data['train'][:])[['tokens','ner_tags_num']].iloc[0]"
      ],
      "metadata": {
        "id": "-HsKvQbnafvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = data['train'].features['ner_tags']\n",
        "\n",
        "{idx:tag for idx, tag in enumerate(tags.feature.names)}"
      ],
      "metadata": {
        "id": "rwKtJs_abzt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['train'].features)"
      ],
      "metadata": {
        "id": "ZG-woOmxcuuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract all unique NER tag strings\n",
        "all_tag_strings = [tag for example in data['train']['ner_tags'] for tag in example]\n",
        "unique_tags = sorted(set(all_tag_strings))\n",
        "\n",
        "# Step 2: Create mappings\n",
        "label_to_id = {label: idx for idx, label in enumerate(unique_tags)}\n",
        "id_to_label = {idx: label for label, idx in label_to_id.items()}"
      ],
      "metadata": {
        "id": "2nPdjPkcc8VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "\n",
        "# Define the class label with your tag list (must be sorted to match IDs if already mapped)\n",
        "class_label = ClassLabel(names=unique_tags)\n",
        "\n",
        "# Cast the string-based 'ner_tags' column to ClassLabel (automatically maps strings to ints)\n",
        "data = data.cast_column(\"ner_tags\", Sequence(class_label))"
      ],
      "metadata": {
        "id": "0mNhH0n9dd4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check type of ner_tags feature\n",
        "print(data['train'].features['ner_tags'])"
      ],
      "metadata": {
        "id": "lZk6uxCbdy4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Print one example row: should now show integer IDs\n",
        "print(data['train'][0]['ner_tags'])  # -> list of ints, e.g. [0, 1, 1, 2, ...]"
      ],
      "metadata": {
        "id": "4EIqsVCLd8eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = data['train'].features['ner_tags'].feature\n",
        "\n",
        "index2tag = {idx:tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag:idx for idx, tag in enumerate(tags.names)}"
      ],
      "metadata": {
        "id": "hKw4spiYekDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags.feature.names"
      ],
      "metadata": {
        "id": "nRUWGjTueyqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(data['train'][:])[['tokens', 'ner_tags']].iloc[0]"
      ],
      "metadata": {
        "id": "8s0wRXm8fVZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag"
      ],
      "metadata": {
        "id": "U66_1SftfX6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2index"
      ],
      "metadata": {
        "id": "L-PFFO6vfv6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags.int2str(3)"
      ],
      "metadata": {
        "id": "2sPBPBqjfy_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tag_names(batch):\n",
        "  tag_name = {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n",
        "  return tag_name\n",
        "\n",
        "data = data.map(create_tag_names)\n",
        "\n",
        "print(data)\n",
        "print(pd.DataFrame(data['train'][:])[['tokens', 'ner_tags', 'ner_tags_str']].iloc[0])"
      ],
      "metadata": {
        "id": "mrF5REnFf8wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model building"
      ],
      "metadata": {
        "id": "LzJGBqdBkFjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"distilbert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "RafmmSfvkKGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.is_fast"
      ],
      "metadata": {
        "id": "svNi96nGkpv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = data['train'][0]['tokens']\n",
        "inputs = tokenizer(inputs, is_split_into_words=True)\n",
        "inputs.tokens()"
      ],
      "metadata": {
        "id": "0At8r_VolCRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['train'][0]['tokens']"
      ],
      "metadata": {
        "id": "MAvkdJMblLNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.word_ids()"
      ],
      "metadata": {
        "id": "xT8iZkz_lO9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "  new_labels = []\n",
        "  current_word=None\n",
        "  for word_id in word_ids:\n",
        "    if word_id != current_word:\n",
        "      current_word = word_id\n",
        "      label = -100 if word_id is None else labels[word_id]\n",
        "      new_labels.append(label)\n",
        "\n",
        "    elif word_id is None:\n",
        "      new_labels.append(-100)\n",
        "\n",
        "    else:\n",
        "      label = labels[word_id]\n",
        "\n",
        "      if label%2==1:\n",
        "        label = label + 1\n",
        "      new_labels.append(label)\n",
        "\n",
        "  return new_labels"
      ],
      "metadata": {
        "id": "cwaEIDOzl1bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data['train'][0]['ner_tags']\n",
        "word_ids = inputs.word_ids()\n",
        "print(labels, word_ids)"
      ],
      "metadata": {
        "id": "ROmvu6RcmWCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "align_labels_with_tokens(labels, word_ids)"
      ],
      "metadata": {
        "id": "XTHm7x2Bmi-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "  tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
        "\n",
        "  all_labels = examples['ner_tags']\n",
        "\n",
        "  new_labels = []\n",
        "  for i, labels in enumerate(all_labels):\n",
        "    word_ids = tokenized_inputs.word_ids(i)\n",
        "    new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "  tokenized_inputs['labels'] = new_labels\n",
        "\n",
        "  return tokenized_inputs"
      ],
      "metadata": {
        "id": "kSw8b2NUmkZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, remove_columns=data['train'].column_names)"
      ],
      "metadata": {
        "id": "fpUdGm-fn29p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "QHV4Lc1IvRDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data collation"
      ],
      "metadata": {
        "id": "H_16-oy7vjNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "eyn7Pk01vev3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
        "batch"
      ],
      "metadata": {
        "id": "qQuWeMyKwqFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## metrics"
      ],
      "metadata": {
        "id": "lFWNF3ci1D7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n",
        "!pip install evaluate\n",
        "\n",
        "import evaluate\n",
        "metric = evaluate.load('seqeval')"
      ],
      "metadata": {
        "id": "borw1RP6xu_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_feature = data['train'].features['ner_tags']\n",
        "ner_feature"
      ],
      "metadata": {
        "id": "Pb8RAdN420VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_names = ner_feature.feature.names\n",
        "label_names"
      ],
      "metadata": {
        "id": "1oCjj7TD3FAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data['train'][0]['ner_tags']\n",
        "labels = [label_names[i] for i in labels]\n",
        "labels"
      ],
      "metadata": {
        "id": "AY7cSsf53NnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = labels.copy()\n",
        "predictions[2] = \"Other\"\n",
        "\n",
        "metric.compute(predictions=[predictions], references=[labels])"
      ],
      "metadata": {
        "id": "Ny3jO99x3qCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "  logits, labels = eval_preds\n",
        "\n",
        "  predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "  true_labels = [[label_names[l] for l in label if l!=-100] for label in labels]\n",
        "\n",
        "  true_predictions = [[label_names[l] for p,l in zip(prediction, label) if l!=-100]\n",
        "                      for prediction, label in zip(predictions, labels)]\n",
        "\n",
        "  all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n",
        "  return all_metrics"
      ],
      "metadata": {
        "id": "7EM8h40e5x5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model training"
      ],
      "metadata": {
        "id": "_SHr53gT9ELe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {i:label for i, label in enumerate(label_names)}\n",
        "label2id = {label:i for i, label in enumerate(label_names)}"
      ],
      "metadata": {
        "id": "vVUZtfUP9SVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(id2label)"
      ],
      "metadata": {
        "id": "qpYNt_Wf9xHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "sTqAu6pm-zll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, id2label=index2tag)"
      ],
      "metadata": {
        "id": "DOkPWnMj9C_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.num_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZNuQKKs939K",
        "outputId": "d8d6ad91-9d9a-402c-a252-78c70084595c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\"distilbert-finetuned-ner\",\n",
        "                        # evaluation_strategy = \"epoch\",\n",
        "                         save_strategy=\"epoch\",\n",
        "                         learning_rate = 2e-5,\n",
        "                         num_train_epochs=3,\n",
        "                         weight_decay=0.01)"
      ],
      "metadata": {
        "id": "pVF2jKHx9z_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# from datasets import Dataset\n",
        "\n",
        "# # Convert the dataset to a pandas DataFrame\n",
        "# df = pd.DataFrame(data['train'])\n",
        "\n",
        "# # Split the dataset into train and validation\n",
        "# train_df, val_df = train_test_split(df, test_size=0.1)  # 90% train, 10% validation\n",
        "\n",
        "# # Convert DataFrames back to Hugging Face Dataset format\n",
        "# train_dataset = Dataset.from_pandas(train_df)\n",
        "# val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# # Define training arguments with evaluation strategy\n",
        "# from transformers import TrainingArguments\n",
        "\n",
        "# args = TrainingArguments(\n",
        "#     output_dir=\"distilbert-finetuned-ner\",\n",
        "#     evaluation_strategy=\"epoch\",  # Evaluate every epoch\n",
        "#     save_strategy=\"epoch\",       # Save model every epoch\n",
        "#     learning_rate=2e-5,          # Learning rate\n",
        "#     num_train_epochs=3,          # Number of training epochs\n",
        "#     weight_decay=0.01            # Weight decay\n",
        "# )\n",
        "\n",
        "# from transformers import Trainer\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,  # Pre-trained or fine-tuned model\n",
        "#     args=args,     # Training arguments\n",
        "#     train_dataset=train_dataset,  # Training data\n",
        "#     eval_dataset=val_dataset,    # Validation data\n",
        "#     compute_metrics=compute_metrics  # Metrics function\n",
        "# )\n",
        "\n",
        "# trainer.train()  # Start training"
      ],
      "metadata": {
        "id": "iO7BFPriByQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7vYbAV0pdPdQyGLWlOj2b",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}