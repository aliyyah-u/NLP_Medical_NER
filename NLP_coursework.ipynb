{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliyyah-u/NLP_Medical_NER/blob/main/NLP_coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IbzjCqsrupf0"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Aliyyah U.\"\n",
        "__version__ = \"IN3045 City St George's, University of London, Spring 2025\"\n",
        "#Ideally, we would reuse trusted libraries to perform NLP. Some of the commonly recommended ones are: , nltk, spacy, torchtext, huggingface: transformers, datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEAN"
      ],
      "metadata": {
        "id": "IzQOY-CJZXcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE MODEL"
      ],
      "metadata": {
        "id": "4uZxvm7zPkyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Log in to Huggingface Hub (make sure you have valid credentials)\n",
        "!pip install -U huggingface_hub\n",
        "login()\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "print('\\nDATASET FEATURES:' + '\\n', dataset)\n",
        "\n",
        "# See dataset samples\n",
        "print('\\nA DATASET SAMPLE:')\n",
        "print(dataset[\"train\"][0][\"tokens\"])  # Can see that dataset is already tokenized\n",
        "print(dataset[\"train\"][0][\"ner_tags\"])  # Can see that tags are already in BIO format\n",
        "\n",
        "# Split dataset for testing\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "print('\\nTHE SPLIT DATASET FEATURES:' + '\\n', dataset)\n",
        "\n",
        "# See type\n",
        "ner_feature = dataset[\"train\"].features\n",
        "print('\\nDATA TYPES:' + '\\n', ner_feature)\n",
        "\n",
        "# Convert dataset contents into lists for further processing\n",
        "df = dataset[\"train\"].to_pandas()\n",
        "train_tokens = df[\"tokens\"].tolist()\n",
        "train_tags = df[\"ner_tags\"].tolist()\n",
        "\n",
        "# See all unique tag names\n",
        "train_unique_tags = set(tag for sublist in train_tags for tag in sublist)\n",
        "print(\"\\nALL UNIQUE NER TAGS IN TRAINING SET:\")\n",
        "print(sorted(train_unique_tags))\n",
        "\n",
        "# View samples\n",
        "print(\"\\nSAMPLE TRAINING TOKENS:\")\n",
        "print(train_tokens[0])\n",
        "print(\"\\nSAMPLE TRAINING TAGS:\")\n",
        "print(train_tags[0])\n",
        "\n",
        "# BASELINE: Function tags each token with \"Other\"\n",
        "def add_predicted_tags(tokens, tags):\n",
        "    predicted_tags = [['Other'] * len(token_list) for token_list in tags]\n",
        "    return predicted_tags\n",
        "\n",
        "# Add predicted NER tags to the dataset\n",
        "predicted_train_tags = add_predicted_tags(train_tokens, train_tags)\n",
        "# Now, add predicted_ner_tags to the original dataset\n",
        "dataset[\"train\"] = dataset[\"train\"].add_column(\"predicted_ner_tags\", predicted_train_tags)\n",
        "\n",
        "# Print a sample of tokens and their true and predicted NER tags\n",
        "def match_tokens_labels(tokens, true_labels, predicted_labels):\n",
        "    line1 = \"\"\n",
        "    line2 = \"\"\n",
        "    line3 = \"\"\n",
        "    # Iterate over tokens, true labels, and predicted labels together\n",
        "    for word, true_label, predicted_label in zip(tokens, true_labels, predicted_labels):\n",
        "        max_length = max(len(word), len(true_label), len(predicted_label))\n",
        "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "        line2 += true_label + \" \" * (max_length - len(true_label) + 1)\n",
        "        line3 += predicted_label + \" \" * (max_length - len(predicted_label) + 1)\n",
        "    # Print the output with correct alignment\n",
        "    print('\\nSAMPLE OF TOKENS WITH TRUE AND PREDICTED NER TAGS' + '\\n', line1)\n",
        "    print(line2)\n",
        "    print(line3)\n",
        "\n",
        "# Match tokens, true NER tags, and predicted \"Other\" labels for the first sample\n",
        "match_tokens_labels(train_tokens[0], train_tags[0], predicted_train_tags[0])\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Flatten true and predicted tags\n",
        "flat_true = [tag for sent in train_tags for tag in sent]\n",
        "flat_pred = [tag for sent in predicted_train_tags for tag in sent]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(flat_true, flat_pred)\n",
        "print(f\"\\nBaseline Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Compute report excluding \"Other\"\n",
        "labels = sorted(set(flat_true) - {\"Other\"})\n",
        "report = classification_report(flat_true, flat_pred, labels=labels, zero_division=0)\n",
        "print(\"\\nBaseline Classification Report (excluding 'Other'):\\n\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "DtbCBgO5PlD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Messy"
      ],
      "metadata": {
        "id": "Jo4_3i-cKNnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "ickVgEWwQs-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "from datasets import load_dataset\n",
        "!pip install -U huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "dataset = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "print('\\nDATASET FEATURES:'+'\\n',dataset)\n",
        "\n",
        "#see dataset samples\n",
        "print('\\nA DATASET SAMPLE:')\n",
        "print(dataset[\"train\"][0][\"tokens\"]) #can see that dataset is already tokenised\n",
        "print(dataset[\"train\"][0][\"ner_tags\"]) #can see that tags already in BIO format\n",
        "\n",
        "#split dataset for testing\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "print('\\nTHE SPLIT DATASET FEATURES:'+'\\n', dataset)\n",
        "\n",
        "#see type\n",
        "ner_feature = dataset[\"train\"].features\n",
        "print('\\nDATA TYPES:'+'\\n',ner_feature)\n",
        "\n",
        "#convert dataset contents into lists for further processing\n",
        "df = dataset[\"train\"].to_pandas()\n",
        "train_tokens = df[\"tokens\"].tolist()\n",
        "train_tags = df[\"ner_tags\"].tolist()\n",
        "\n",
        "# See all unique tag names\n",
        "train_unique_tags = set(tag for sublist in train_tags for tag in sublist)\n",
        "print(\"\\nALL UNIQUE NER TAGS IN TRAINING SET:\")\n",
        "print(sorted(train_unique_tags))\n",
        "\n",
        "#view samples\n",
        "print(\"\\nSAMPLE TRAINING TOKENS:\")\n",
        "print(train_tokens[:2])\n",
        "print(\"\\nSAMPLE TRAINING TAGS:\")\n",
        "print(train_tags[:2])\n",
        "\n",
        "#print sample of tokens with correct ner_tags\n",
        "words = dataset[\"train\"][0][\"tokens\"]\n",
        "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
        "\n",
        "def match_tokens_labels(tokens, labels):\n",
        "  line1 = \"\"\n",
        "  line2 = \"\"\n",
        "  for word, label in zip(words, labels):\n",
        "    max_length = max(len(word), len(label))\n",
        "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "    line2 += label + \" \" * (max_length - len(label) + 1)\n",
        "  print('\\nSAMPLE OF TOKENS WITH CORRECT MATCHING NER TAGS'+'\\n',line1)\n",
        "  print(line2)\n",
        "\n",
        "match_tokens_labels(words, labels)"
      ],
      "metadata": {
        "id": "6CIiSsAFZ8vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Create a dictionary to store tokens by their NER tags\n",
        "ner_groups = defaultdict(list)\n",
        "\n",
        "# Loop through all tokens and their corresponding NER tags\n",
        "for tokens, tags in zip(train_tokens, train_tags):\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        # Only add tokens that aren't tagged with \"Other\"\n",
        "        if tag != 'Other':\n",
        "            ner_groups[tag].append(token)\n",
        "\n",
        "# Display tokens grouped by their NER tag\n",
        "for tag, tokens in sorted(ner_groups.items()):\n",
        "    print(f\"\\nNER Tag: {tag}\")\n",
        "    print(f\"Tokens: {', '.join(sorted(set(tokens)))}\")  # Remove duplicates with set and sort the tokens"
      ],
      "metadata": {
        "id": "C-FbP_HeMexm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Create a dictionary to store tokens by their NER tags\n",
        "ner_groups = defaultdict(list)\n",
        "\n",
        "# Loop through all tokens and their corresponding NER tags\n",
        "for tokens, tags in zip(train_tokens, train_tags):\n",
        "    for token, tag in zip(tokens, tags):\n",
        "        # Only add tokens that aren't tagged with \"Other\"\n",
        "        if tag != 'Other':\n",
        "            ner_groups[tag].append(token)\n",
        "\n",
        "# Convert the grouped data into a DataFrame\n",
        "data = {'NER_Tag': list(ner_groups.keys()), 'Tokens': [', '.join(sorted(set(tokens))) for tokens in ner_groups.values()]}\n",
        "mydf = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(mydf)"
      ],
      "metadata": {
        "id": "xJllSg-7OeDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## complete preprocessing code"
      ],
      "metadata": {
        "id": "KqNKog0fqKeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "print('\\nDATASET FEATURES:\\n', dataset)\n",
        "\n",
        "# See dataset samples\n",
        "print('\\nA DATASET SAMPLE:')\n",
        "print(dataset[\"train\"][0][\"tokens\"])  # Tokenized dataset\n",
        "print(dataset[\"train\"][0][\"ner_tags\"])  # BIO-formatted tags\n",
        "\n",
        "# Split dataset for testing\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "print('\\nTHE SPLIT DATASET FEATURES:\\n', dataset)\n",
        "\n",
        "# Convert dataset contents into lists for further processing\n",
        "df = dataset[\"train\"].to_pandas()\n",
        "train_tokens = df[\"tokens\"].tolist()\n",
        "train_tags = df[\"ner_tags\"].tolist()\n",
        "\n",
        "# See all unique tag names\n",
        "train_unique_tags = set(tag for sublist in train_tags for tag in sublist)\n",
        "print(\"\\nALL UNIQUE NER TAGS IN TRAINING SET:\")\n",
        "print(sorted(train_unique_tags))\n",
        "\n",
        "# View samples of tokens and tags\n",
        "print(\"\\nSAMPLE TRAINING TOKENS:\", train_tokens[:2])\n",
        "print(\"\\nSAMPLE TRAINING TAGS:\", train_tags[:2])\n",
        "\n",
        "# Label Encoding for ner_tags\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(sorted(train_unique_tags))\n",
        "encoded_tags = [label_encoder.transform(tags) for tags in train_tags]\n",
        "df['encoded_ner_tags'] = encoded_tags\n",
        "print(df[['tokens', 'ner_tags', 'encoded_ner_tags']].head())\n",
        "\n",
        "# Optionally, print the unique encoded tags\n",
        "print(\"Unique encoded tags:\", label_encoder.classes_)\n",
        "\n",
        "# Function to match tokens and their labels\n",
        "def match_tokens_labels(tokens, labels):\n",
        "    line1 = \"\"\n",
        "    line2 = \"\"\n",
        "    for word, label in zip(tokens, labels):\n",
        "        max_length = max(len(word), len(label))\n",
        "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
        "    print('\\nSAMPLE OF TOKENS WITH CORRECT MATCHING NER TAGS\\n', line1)\n",
        "    print(line2)\n",
        "\n",
        "# Display a sample of tokens with labels\n",
        "words = dataset[\"train\"][0][\"tokens\"]\n",
        "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
        "match_tokens_labels(words, labels)\n",
        "\n",
        "#POS TAGGING!\n",
        "# Initialise the spaCy model for POS tagging\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to get POS tags and process the tokens\n",
        "def get_pos_tags(tokens):\n",
        "    # Join the tokens into a single string for processing with spaCy\n",
        "    sentence = \" \".join(tokens)\n",
        "\n",
        "    # Process the sentence using spaCy's pipeline\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # Extract POS tags for each token\n",
        "    pos_tags = [(token.text, token.pos_, token.lemma_, token.tag_, token.dep_) for token in doc]\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "# Example: Process the first training sample\n",
        "tokens_example = train_tokens[0]\n",
        "pos_tags_example = get_pos_tags(tokens_example)\n",
        "\n",
        "# Print the POS tags for the first sample\n",
        "print(\"\\nPOS Tags for the first sample:\")\n",
        "for token, pos, lemma, tag, dep in pos_tags_example:\n",
        "    print(f\"Token: {token}, POS: {pos}, Lemma: {lemma}, Tag: {tag}, Dependency: {dep}\")"
      ],
      "metadata": {
        "id": "lAbtwjQEblDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"parsa-mhmdi/Medical_NER\")\n",
        "print('\\nDATASET FEATURES:\\n', dataset)\n",
        "\n",
        "# See dataset samples\n",
        "print('\\nA DATASET SAMPLE:')\n",
        "print(dataset[\"train\"][0][\"tokens\"])  # Tokenized dataset\n",
        "print(dataset[\"train\"][0][\"ner_tags\"])  # BIO-formatted tags\n",
        "\n",
        "# Split dataset for testing\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "print('\\nTHE SPLIT DATASET FEATURES:\\n', dataset)\n",
        "\n",
        "# Convert dataset contents into lists for further processing\n",
        "df = dataset[\"train\"].to_pandas()\n",
        "train_tokens = df[\"tokens\"].tolist()\n",
        "train_tags = df[\"ner_tags\"].tolist()\n",
        "\n",
        "# See all unique tag names\n",
        "train_unique_tags = set(tag for sublist in train_tags for tag in sublist)\n",
        "print(\"\\nALL UNIQUE NER TAGS IN TRAINING SET:\")\n",
        "print(sorted(train_unique_tags))\n",
        "\n",
        "# View samples of tokens and tags\n",
        "print(\"\\nSAMPLE TRAINING TOKENS:\", train_tokens[:2])\n",
        "print(\"\\nSAMPLE TRAINING TAGS:\", train_tags[:2])\n",
        "\n",
        "# Label Encoding for ner_tags\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(sorted(train_unique_tags))\n",
        "encoded_tags = [label_encoder.transform(tags) for tags in train_tags]\n",
        "df['encoded_ner_tags'] = encoded_tags\n",
        "print(df[['tokens', 'ner_tags', 'encoded_ner_tags']].head())\n",
        "\n",
        "# Optionally, print the unique encoded tags\n",
        "print(\"Unique encoded tags:\", label_encoder.classes_)\n",
        "\n",
        "# Function to match tokens and their labels\n",
        "def match_tokens_labels(tokens, labels):\n",
        "    line1 = \"\"\n",
        "    line2 = \"\"\n",
        "    for word, label in zip(tokens, labels):\n",
        "        max_length = max(len(word), len(label))\n",
        "        line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "        line2 += label + \" \" * (max_length - len(label) + 1)\n",
        "    print('\\nSAMPLE OF TOKENS WITH CORRECT MATCHING NER TAGS\\n', line1)\n",
        "    print(line2)\n",
        "\n",
        "# Display a sample of tokens with labels\n",
        "words = dataset[\"train\"][0][\"tokens\"]\n",
        "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
        "match_tokens_labels(words, labels)\n",
        "\n",
        "#POS TAGGING!\n",
        "# Initialise the spaCy model for POS tagging\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to get POS tags and process the tokens\n",
        "def get_pos_tags_aligned(tokens):\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    spacy_tokens = [token.text for token in doc]\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "    aligned_tags = []\n",
        "    idx = 0\n",
        "    for tok in tokens:\n",
        "        # Try matching token from original with spacy's token stream\n",
        "        if idx < len(spacy_tokens) and tok == spacy_tokens[idx]:\n",
        "            aligned_tags.append(pos_tags[idx])\n",
        "            idx += 1\n",
        "        else:\n",
        "            # Fallback: assign \"X\" if alignment fails\n",
        "            aligned_tags.append(\"X\")\n",
        "\n",
        "    return aligned_tags\n",
        "\n",
        "# Example: Process the first training sample\n",
        "tokens_example = train_tokens[0]\n",
        "pos_tags_example = get_pos_tags(tokens_example)\n",
        "\n",
        "# Print the POS tags for the first sample\n",
        "print(\"\\nPOS Tags for the first sample:\")\n",
        "for token, pos, lemma, tag, dep in pos_tags_example:\n",
        "    print(f\"Token: {token}, POS: {pos}, Lemma: {lemma}, Tag: {tag}, Dependency: {dep}\")\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# === Create a mapping from encoded tag IDs to tag names ===\n",
        "label_encoder = LabelEncoder()\n",
        "all_tags = df[\"ner_tags\"].explode().unique()\n",
        "label_encoder.fit(sorted(all_tags))\n",
        "label_map = dict(enumerate(label_encoder.classes_))\n",
        "\n",
        "# === Rule-based NER function using POS tags ===\n",
        "def rule_based_ner(tokens):\n",
        "    pos_tags = get_pos_tags_aligned(tokens)\n",
        "    predicted_labels = []\n",
        "\n",
        "    for pos in pos_tags:\n",
        "        if pos in [\"PROPN\", \"NOUN\"]:\n",
        "            predicted_labels.append(\"B-MED\")\n",
        "        else:\n",
        "            predicted_labels.append(\"O\")\n",
        "\n",
        "    return predicted_labels\n",
        "\n",
        "# === Evaluate rule-based NER on first 50 training samples ===\n",
        "true_labels_flat = []\n",
        "pred_labels_flat = []\n",
        "\n",
        "for i in range(50):\n",
        "    tokens = train_tokens[i]\n",
        "    true_ids = encoded_tags[i]\n",
        "    true_tags = label_encoder.inverse_transform(true_ids)\n",
        "\n",
        "    pred_tags = rule_based_ner(tokens)\n",
        "\n",
        "    true_labels_flat.extend(true_tags)\n",
        "    pred_labels_flat.extend(pred_tags)\n",
        "\n",
        "# === Classification report ===\n",
        "print(\"\\nRule-Based Baseline NER Results (first 50 samples):\")\n",
        "print(classification_report(true_labels_flat, pred_labels_flat))"
      ],
      "metadata": {
        "id": "nZzCsrQnFr6k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IzQOY-CJZXcx",
        "4uZxvm7zPkyB",
        "Jo4_3i-cKNnE"
      ],
      "authorship_tag": "ABX9TyP4Zh2NY6lK4To6vZNzvG7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}